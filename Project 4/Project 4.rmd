---
title: "607 Project 4"
author: "Lu Beyer"
output:
  html_document: default
  pdf_document: default
---
# INSTRUCTIONS
It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).


I downloaded and used a sms spam dataset collected by the UC Irvine Machine Learning Repository: 
https://archive.ics.uci.edu/dataset/228/sms+spam+collection


First we load our libraries, and then load our data through read.table(), setting sep "\t" to note that records are separated by tabs.
We then rename and select our relevant columns for use.
```{r}
library(tidyverse)
library(tm)
library(SnowballC)
library(e1071)

#load data
data <- read.table("SMSSpamCollection", sep = "\t", stringsAsFactors = FALSE)

#renaming and selecting relevant columns
a <- data %>% 
  rename(txt = V2,
         flag = V1) %>% 
  select(txt, flag)
```
Using tm::Corpus, we convert our data into a corpus.
```{r}
#converting data to corpus
text_corpus <- Corpus(VectorSource(a$txt), readerControl = list(language = "en", encoding = "UTF-8"))
text_corpus
```

tm_map allows us to perform various standard text cleaning operations, such as removing stopwords, whitespaces, and stemming, which is reduces a word to its root.  ie:  "running" would be reduced to "run", for easier, more consistent text parsing. 
```{r}
#removing whitespace, converting text to lowercase, removing stopwords,
#removing punctionation, numbers, and stemming words within corpus
b <- text_corpus %>% 
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(tolower)) %>% 
  tm_map(removeWords, stopwords("english")) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(stemDocument)
```


Using tm to create a Document Term Matrix (dtm)
```{r}
#create document term matrix using tm
dtm <- DocumentTermMatrix(b)

dtm
```
We then set our sample size to 75% of the records to train our model on, and identify those records.
```{r}
#set sample size, set indices
samp <-  round(0.75 * nrow(dtm))
indices_train <- sample(seq_len(nrow(dtm)), size = samp)
```

```{r}
#split data based on indices
train_data <-  dtm[indices_train, ]
test_data  <-  dtm[-indices_train, ] 
```

```{r}
#extract labels based on indices
train_label <-  a[indices_train, ]$flag
test_label  <-  a[-indices_train, ]$flag
```

We then use the identified training data to train our model
```{r}
#train model 
nb <- naiveBayes(as.matrix(train_data), as.factor(train_label))
```

Using our trained model, we predict the the results of the remaining 25% of records
```{r}
#predict new test data labels based on trained model
result <- predict(nb, newdata = as.matrix(test_data))
```


```{r}
#create and print confusion matrix based on test results
cm <- table(Predicted = result, Actual = test_label)
print(cm)
```



